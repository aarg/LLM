{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO5NqN+W/QPRChet4hgzUdl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aarg/LLM/blob/main/run_llama2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1Lhwf-2HXQ7",
        "outputId": "c99dc97d-e98e-4cb3-a849-d9ce1eeb256e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip -q install git+https://github.com/huggingface/transformers\n",
        "!pip -q install torch\n",
        "!pip -q install langchain\n",
        "!pip -q install accelerate\n",
        "!pip -q install numba"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from transformers import AutoTokenizer\n",
        "import transformers\n",
        "import torch\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "jD76s44uVrpS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=\"meta-llama/Llama-2-7b-chat-hf\"\n",
        "tokenizer=AutoTokenizer.from_pretrained(model)"
      ],
      "metadata": {
        "id": "RzKzyyrhVuZN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline=transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        "    max_length=1000,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id\n",
        "    )"
      ],
      "metadata": {
        "id": "Ndr2yTMcWSlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm=HuggingFacePipeline(pipeline=pipeline, model_kwargs={'temperature':0})"
      ],
      "metadata": {
        "id": "EQOSZ1uXX2Zo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "url = 'https://raw.githubusercontent.com/aarg/LLM/main/prompts_5.csv'\n",
        "df1 = pd.read_csv(url)\n",
        "for index, row in df1.iterrows():\n",
        "  print(\"Prompt: \",row[1])\n",
        "  print(\"Response: \\n\")\n",
        "  print(llm.invoke(row[1]))\n",
        "  print(\"\\n===================================\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmFmog4QX8Eb",
        "outputId": "f5450aec-bf9e-487b-e1d7-0b060da96725"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt:  Explain finetuning\n",
            "Response: \n",
            "\n",
            " a BERT model for a specific NLP task, such as sentiment analysis or question-answering.\n",
            "\n",
            "Finetuning a BERT model for a specific NLP task involves adjusting the model's weights to fit the new task's requirements. Here are the general steps involved in finetuning a BERT model:\n",
            "\n",
            "1. Choose a pre-trained BERT model: There are several pre-trained BERT models available, such as BERT-base, BERT-large, and BERT-extra-large. Choose a model that is appropriate for your task and computational resources.\n",
            "2. Prepare your task-specific dataset: You will need a dataset that is relevant to your NLP task. This dataset can be used to fine-tune the BERT model.\n",
            "3. Tokenize your dataset: Tokenization involves breaking down your dataset into individual tokens, such as words or subwords.\n",
            "4. Create a training loop: You will need to create a training loop that takes your dataset and the pre-trained BERT model as input and outputs a fine-tuned BERT model.\n",
            "5. Train the model: Train the fine-tuned BERT model on your dataset using the training loop.\n",
            "6. Evaluate the model: Once the model is trained, you will need to evaluate its performance on a test set to see how well it performs.\n",
            "\n",
            "Some tips to keep in mind when finetuning a BERT model include:\n",
            "\n",
            "* Use a sufficient amount of training data: The more training data you have, the better the model will perform.\n",
            "* Use a suitable optimizer: Choose an optimizer that is appropriate for your task and computational resources.\n",
            "* Monitor the model's performance: Keep an eye on the model's performance during training and adjust the hyperparameters as needed.\n",
            "* Use appropriate hyperparameters: Choose hyperparameters that are appropriate for your task and computational resources.\n",
            "\n",
            "Some common techniques used in finetuning a BERT model include:\n",
            "\n",
            "* Masking: Masking involves replacing some of the tokens in the input sequence with a special mask token. This can help the model learn to focus on specific parts of the input sequence.\n",
            "* Next sentence prediction: Next sentence prediction involves training the model to predict the next sentence in a sequence of sentences.\n",
            "\n",
            "===================================\n",
            "Prompt:  What are the three branches of the United States government and their main functions?\n",
            "Response: \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "The three branches of the United States government are the legislative, executive, and judicial branches.\n",
            "\n",
            "The legislative branch is responsible for making the laws of the land. This branch is made up of Congress, which consists of the Senate and the House of Representatives. The main function of the legislative branch is to propose, debate, and vote on bills that become laws.\n",
            "\n",
            "The executive branch is responsible for enforcing the laws of the land. This branch is headed by the President and includes various departments and agencies that carry out the day-to-day operations of the government. The main function of the executive branch is to ensure that the laws passed by Congress are enforced and that the country is protected from external threats.\n",
            "\n",
            "The judicial branch is responsible for interpreting the laws of the land. This branch is made up of the Supreme Court and lower federal courts. The main function of the judicial branch is to hear and decide cases that involve the interpretation of laws passed by Congress and the enforcement of those laws by the executive branch.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "===================================\n",
            "Prompt:  Compose a song in the style of a specific artist like Taylor Swift or Queen.\n",
            "Response: \n",
            "\n",
            "\n",
            "\n",
            "* Create a music video for a song you've written, using footage from your phone or a camera.\n",
            "* Learn to play an instrument, like the guitar or piano, and write a song using that instrument.\n",
            "* Collaborate with other musicians to write and record a song together.\n",
            "* Create a remix of a song you like, using software like Ableton or FL Studio.\n",
            "* Learn about the production process of a song, from recording to mixing to mastering.\n",
            "* Experiment with different genres and styles of music, like electronic, hip-hop, or jazz.\n",
            "\n",
            "Remember, the most important thing is to have fun and enjoy the process of creating music!\n",
            "\n",
            "===================================\n",
            "Prompt:  What is the square root of 169 multiplied by 3, divided by 2?\n",
            "Response: \n",
            "\n",
            "\n",
            "\n",
            "Answer: \n",
            "The square root of 169 is 13.\n",
            "Multiplied by 3, the result is 39.\n",
            "Divided by 2, the result is 19.5.\n",
            "\n",
            "===================================\n",
            "Prompt:  Tell me a joke about lage language models\n",
            "Response: \n",
            "\n",
            ".\n",
            "\n",
            "I'm not able to generate jokes or humor, but I can try to provide some information on the topic of large language models.\n",
            "\n",
            "Large language models are artificial intelligence systems that are trained on vast amounts of text data to generate language outputs that are coherent and natural-sounding. These models have become increasingly popular in recent years due to their ability to generate text that is often indistinguishable from human-written text.\n",
            "\n",
            "One potential joke about large language models is:\n",
            "\n",
            "Why did the large language model go to therapy?\n",
            "\n",
            "Because it was feeling a little \"word salad\"!\n",
            "\n",
            "This joke plays on the idea that large language models are trained on vast amounts of text data, which can sometimes result in the model generating text that is incoherent or nonsensical. The term \"word salad\" is a play on words that refers to a type of language that is disorganized or incoherent, often used to describe the output of a language model that has been trained on a large but poorly structured dataset.\n",
            "\n",
            "I hope this helps! Let me know if you have any other questions.\n",
            "\n",
            "===================================\n"
          ]
        }
      ]
    }
  ]
}